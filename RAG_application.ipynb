{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLAUDE_API_KEY\n",
      "GOOGLE_API_KEY\n",
      "HUGGINGFACE_API_KEY\n",
      "OPENAI_API_KEY\n",
      "PINECONE_API_KEY\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv ,find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "if os.environ:\n",
    "    for api_key in os.environ:\n",
    "        if \"API_KEY\" in api_key:\n",
    "            print(api_key)\n",
    "else:\n",
    "    import getpass\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"GOOGLE_API_KEY\")\n",
    "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(docs_urls=[\"https://pypi.org/\"]):\n",
    "    from langchain.document_loaders.async_html import AsyncHtmlLoader\n",
    "    print(\"loading started....\")\n",
    "    loader = AsyncHtmlLoader(docs_urls)\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(html_page:str, title:str):\n",
    "    from pprint import pprint\n",
    "    from bs4 import BeautifulSoup\n",
    "    parser = BeautifulSoup(html_page, \"html.parser\")\n",
    "    # pprint(parser.prettify())\n",
    "    with open(f\"files/{title}.txt\", \"w\",encoding=\"utf-8\") as f:\n",
    "        for string in parser.strings:\n",
    "            if string !=\"\\n\":\n",
    "                f.write(string.strip())\n",
    "                f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading started....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 1/1 [00:14<00:00, 14.69s/it]\n"
     ]
    }
   ],
   "source": [
    "urls = [\n",
    "    \"https://fsciences.univ-setif.dz/main_page/english\",\n",
    "]\n",
    "file_titles = []\n",
    "html_pages = load_docs(urls)\n",
    "for i,html_page in enumerate(html_pages):\n",
    "    cleaned_file_title = (\n",
    "        urls[i]\n",
    "        .replace(\"/\", \"_\")\n",
    "        .replace(\".\", \"_\")\n",
    "        .replace(\"-\", \"_\")\n",
    "        .replace(\"https:\", \"\")\n",
    "        .replace(\"dz\", \"\")\n",
    "        .replace(\"net\", \"\")\n",
    "        .replace(\"com\", \"\")\n",
    "        .replace(\"org\", \"\")\n",
    "        .replace(\"edu\", \"\")\n",
    "        .strip(\"_\")\n",
    "    )\n",
    "    clean_html(\n",
    "        html_page.page_content,\n",
    "        cleaned_file_title\n",
    "    )\n",
    "    file_titles.append(cleaned_file_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def chunks_loader(\n",
    "        text:str #, pdfs, docs,....etc\n",
    ")->List[str]:\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=200,\n",
    "        chunk_overlap=20\n",
    "    )\n",
    "    chunks = text_splitter.create_documents([text])# [texts, pdfs, docs,.... ]    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "chunks_list = []\n",
    "for title in file_titles:\n",
    "    with open(f'files/{title}.txt',\"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "        chunks = chunks_loader(text)\n",
    "        chunks_list.append(chunks)\n",
    "        pprint(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_or_fetch_embeddings(chunks,index_name=\"site-web-faculte-science-info\"):\n",
    "    import pinecone\n",
    "    from langchain_community.vectorstores import Pinecone\n",
    "    from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
    "    pc = pinecone.Pinecone()\n",
    "    embedding = GoogleGenerativeAIEmbeddings(model='models/text-embedding-004')\n",
    "    if index_name in pc.list_indexes().names():\n",
    "        print(f\"index {index_name} already exists\"  )\n",
    "        vector_store = Pinecone.from_existing_index(index_name, embedding)\n",
    "    else:\n",
    "        print(f\"creating the index {index_name}\")\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=768,\n",
    "            metric=\"cosine\",\n",
    "            # metric=\"euclidean\",\n",
    "            spec=pinecone.PodSpec(environment=\"gcp-starter\"),\n",
    "        )\n",
    "        print(f\"Done Creation of {index_name}\")\n",
    "\n",
    "    vector_store = Pinecone.from_documents(chunks, embedding, index_name=index_name)\n",
    "    print(f\"completed fetching the index {index_name}\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_indexes(index_name=\"all\"):\n",
    "    import pinecone\n",
    "    pc = pinecone.Pinecone()\n",
    "    if index_name == \"all\":\n",
    "        for index in pc.list_indexes().names():\n",
    "            pc.delete_index(index)\n",
    "        print(\"all indexes has been deleted\")\n",
    "    else:\n",
    "        pc.delete_index(index_name)\n",
    "        print(f\"{index_name} has been deleted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all indexes has been deleted\n"
     ]
    }
   ],
   "source": [
    "delete_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating the index site-web-faculte-science-info\n",
      "Done Creation of site-web-faculte-science-info\n",
      "completed fetching the index site-web-faculte-science-info\n"
     ]
    }
   ],
   "source": [
    "vector_store = insert_or_fetch_embeddings(chunks_list[0])\n",
    "\n",
    "# for chunks in chunks_list:\n",
    "#     insert_or_fetch_embeddings(chunks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "gemini = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=1)\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "while True:\n",
    "    query = input(\"give your query:\")\n",
    "\n",
    "    # enhanced_prompt = PromptTemplate.from_template(\"translate to arabic {query}\")\n",
    "    # chain = LLMChain(llm=gemini, prompt=enhanced_prompt, verbose=True)\n",
    "    # query = chain.invoke(input=query)\n",
    "    if query in [\"quit\", \"exit\"]:\n",
    "        break\n",
    "    top_result = vector_store.similarity_search(query)\n",
    "    # pprint(top_result)\n",
    "    results = [result.page_content for result in top_result]\n",
    "    results = \"\\n\".join(results)\n",
    "    # print(results)\n",
    "    enhanced_prompt = PromptTemplate.from_template(\n",
    "        \"based on this search results:\\n {results} \\ngive a summerizations\"\n",
    "    )\n",
    "    chain = LLMChain(llm=gemini, prompt=enhanced_prompt, verbose=True)\n",
    "    response = chain.invoke(input=top_result)\n",
    "    print(response[\"text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
